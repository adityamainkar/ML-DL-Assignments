{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning is a deep learning technique where a model trained using a dataset (task) is used for training another data set (task). It has shown to allow faster learning with fewer data points. You can read about it at http://cs231n.github.io/transfer-learning/\n",
        "\n",
        "In this homework, you will be implementing transfer learning. \n",
        "\n",
        "1. Train LeNet (taught in class) with MNIST dataset and random initialization and store the weights by using tf.train.Saver().\n",
        "\n",
        "2. Then retrieve the saved weights and use them to train Fashion MNIST data set (https://github.com/zalandoresearch/fashion-mnist).\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(mnist.train.num_examples) # Number of training data\n",
        "print(mnist.test.num_examples) # Number of test data"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55000\n",
            "10000\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# architecture hyper-parameter\n",
        "learning_rate = 0.01\n",
        "training_iters = 20000\n",
        "batch_size = 128\n",
        "display_step = 20\n",
        "\n",
        "n_input = 784 # 28x28 image\n",
        "n_classes = 10 # 1 for each digit [0-9]\n",
        "dropout = 0.75 \n",
        "\ntf.set_random_seed(3457)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, [None, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "print(x.shape, y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(?, 784) (?, 10)\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d(x, W, b, strides=1):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_net(x, weights, biases, dropout):\n",
        "    # reshape input to 28x28 size\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution layer 1\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max pooling\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution layer 2\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max pooling\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "weights = {\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = conv_net(x, weights, biases, keep_prob)\n",
        "print(model)\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Evaluate model\n",
        "correct_model = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"Add_1:0\", shape=(?, 10), dtype=float32)\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\ninit = tf.global_variables_initializer()"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    step = 1\n",
        "    # Keep training until reach max iterations\n",
        "    while step * batch_size < training_iters:\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)   \n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
        "        if step % display_step == 0:\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y,\n",
        "                                                              keep_prob: 1.})\n",
        "            print(\"Iter \" + str(step*batch_size) + \", Loss= \" + \\\n",
        "                  \"{:.3f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "        step += 1\n",
        "    \n",
        "    # Calculate accuracy for 256 mnist test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
        "                                      y: mnist.test.labels[:256],\n",
        "                                      keep_prob: 1.}))\n",
        "    saver.save(sess, \"mymodel/model.ckpt\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 2560, Loss= 3571.960, Training Accuracy= 0.719\n",
            "Iter 5120, Loss= 762.011, Training Accuracy= 0.766\n",
            "Iter 7680, Loss= 488.152, Training Accuracy= 0.812\n",
            "Iter 10240, Loss= 388.361, Training Accuracy= 0.773\n",
            "Iter 12800, Loss= 484.760, Training Accuracy= 0.742\n",
            "Iter 15360, Loss= 556.658, Training Accuracy= 0.719\n",
            "Iter 17920, Loss= 216.785, Training Accuracy= 0.812\n",
            "Testing Accuracy: 0.808594\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.set_random_seed(123445)\n",
        "digits_fashion = True\n",
        "# Read fashion mnist\n",
        "mnist = input_data.read_data_sets('fashion', one_hot=True)\n",
        "with tf.Session() as sess:\n",
        "    if digits_fashion:\n",
        "        print(\"restoring from mymodel/model.ckpt\")\n",
        "        saver.restore(sess, \"mymodel/model.ckpt\")\n",
        "    else:\n",
        "        sess.run(init)\n",
        "    step = 1\n",
        "    # Keep training until reach max iterations\n",
        "    while step * batch_size < training_iters:\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)   \n",
        "        #print(batch_x.shape, batch_y.shape)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
        "        if step % display_step == 0:\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y,\n",
        "                                                              keep_prob: 1.})\n",
        "            print(\"Iter \" + str(step*batch_size) + \", Loss= \" + \\\n",
        "                  \"{:.3f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "        step += 1\n",
        "    \n",
        "    # Calculate accuracy for 256 mnist test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
        "                                      y: mnist.test.labels[:256],\n",
        "                                      keep_prob: 1.}))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting fashion\\train-images-idx3-ubyte.gz\n",
            "Extracting fashion\\train-labels-idx1-ubyte.gz\n",
            "Extracting fashion\\t10k-images-idx3-ubyte.gz\n",
            "Extracting fashion\\t10k-labels-idx1-ubyte.gz\n",
            "restoring from mymodel/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from mymodel/model.ckpt\n",
            "Iter 2560, Loss= 184.307, Training Accuracy= 0.797\n",
            "Iter 5120, Loss= 206.185, Training Accuracy= 0.828\n",
            "Iter 7680, Loss= 174.121, Training Accuracy= 0.789\n",
            "Iter 10240, Loss= 269.649, Training Accuracy= 0.719\n",
            "Iter 12800, Loss= 172.968, Training Accuracy= 0.797\n",
            "Iter 15360, Loss= 112.278, Training Accuracy= 0.797\n",
            "Iter 17920, Loss= 126.019, Training Accuracy= 0.812\n",
            "Testing Accuracy: 0.800781\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.12.2"
    },
    "widgets": {
      "state": {},
      "version": "1.1.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}