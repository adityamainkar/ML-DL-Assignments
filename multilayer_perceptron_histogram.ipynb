{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"optimizer_graph.png\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "We are reading MNIST data from http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learningrate = 0.001\n",
    "nsteps = 100000\n",
    "batchsize = 128\n",
    "displaystep = 100\n",
    "\n",
    "nhidden1 = 256\n",
    "nhidden2 = 256 \n",
    "ninput = 784\n",
    "noutput = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining X and Y as placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, ninput])\n",
    "Y = tf.placeholder(tf.float32, [None, noutput])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = \\\n",
    "{\n",
    "        'h1': tf.Variable(tf.random_normal([ninput, nhidden1])),\n",
    "        'h2': tf.Variable(tf.random_normal([nhidden1, nhidden2])),\n",
    "        'out': tf.Variable(tf.random_normal([nhidden2, noutput]))\n",
    "}\n",
    "\n",
    "biases = \\\n",
    "{\n",
    "    'b1': tf.Variable(tf.random_normal([nhidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([nhidden2])),\n",
    "    'out': tf.Variable(tf.random_normal([noutput]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model using sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiperceptron(x):\n",
    "    l1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    l2 = tf.nn.sigmoid(tf.add(tf.matmul(l1, weights['h2']), biases['b2']))\n",
    "    outl = tf.nn.sigmoid(tf.add(tf.matmul(l2, weights['out']), biases['out']))\n",
    "    return outl\n",
    "    \n",
    "model = multiperceptron(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cost, optimizer and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.histogram(\"weight_1\",weights['h1'])\n",
    "tf.summary.histogram(\"weight_2\",weights['h2'])\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "filename=\"./summary_log/run\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "writer = tf.summary.FileWriter(filename, tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the session \n",
    "This will run the graph and use all the tensors that were previously defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for steps in range(1, nsteps+1):\n",
    "        batch_x, batch_y=mnist.train.next_batch(batchsize)\n",
    "        sess.run(train_min, feed_dict={X:batch_x, Y:batch_y})\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            losscalc, accuracycalc, merged_summary = \\\n",
    "                      sess.run([loss, accuracy, merged_summary_op], feed_dict={X:batch_x, Y:batch_y})\n",
    "            print(\"Training Loss: %0.4f, Accuracy: %0.4f\"%(losscalc, accuracycalc))\n",
    "            writer.add_summary(merged_summary, steps)\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss:\n",
    "In the figure below, you will notice that the loss decreases rapidly initially and then stabilizes. The accuracy is around 95%. \n",
    "\n",
    "<img src=\"results/loss_mnist.png\" height=\"200px\">\n",
    "\n",
    "### Weights: \n",
    "If you compare the histogram of the various weights, you will see that the histogram of weight1 and weight2 both have changed indicating that both layers learnt the data in the process of training. \n",
    "\n",
    "<img src=\"results/weight1_mnist.png\" height=\"200px\">\n",
    "<img src=\"results/weight2_mnist.png\" height=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nteract": {
   "version": "0.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
